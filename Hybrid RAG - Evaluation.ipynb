{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f2a59a",
   "metadata": {},
   "source": [
    "# 1. Load SQUAD Dataset\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f36f58f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 10570\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad', split='validation')\n",
    "\n",
    "print(squad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56320541",
   "metadata": {},
   "source": [
    "# 2. Imports for Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a13f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "from bert_score import BERTScorer\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c8ebe4",
   "metadata": {},
   "source": [
    "# 3. Evaluate Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de9864e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:02<00:00, 43.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.00%\n",
      "F1 Score: 0.38\n",
      "Precision: 0.41\n",
      "Recall: 0.38\n",
      "BLEU Score: 0.4937353411859368\n",
      "ROUGE-L: 0.6577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load the SentenceTransformer model for retrieval\n",
    "retrieval_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the QA pipeline with the \"deepset/roberta-base-squad2\" model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", device=0)\n",
    "\n",
    "# Load the SQuAD dataset\n",
    "squad = load_dataset('squad', split='validation')\n",
    "squad = squad.select(range(100))\n",
    "\n",
    "# Prepare storage for predictions and ground truth\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Evaluate the model on the SQuAD dataset\n",
    "for example in tqdm(squad):\n",
    "    question = example['question']\n",
    "    context = example['context']\n",
    "    reference_answer = example['answers']['text'][0]\n",
    "\n",
    "    # Step 1: Retrieve relevant passages using SentenceTransformer\n",
    "    query_embedding = retrieval_model.encode(question, convert_to_tensor=True)\n",
    "    context_embedding = retrieval_model.encode(context, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity between question and context\n",
    "    similarity_score = util.pytorch_cos_sim(query_embedding, context_embedding).item()\n",
    "\n",
    "    # Step 2: Use the QA pipeline to predict the answer\n",
    "    if similarity_score > 0.5:  # Threshold to determine if context is relevant\n",
    "        result = qa_pipeline({'question': question, 'context': context})\n",
    "        predicted_answer = result['answer']\n",
    "    else:\n",
    "        predicted_answer = \"\"\n",
    "\n",
    "    predictions.append(predicted_answer)\n",
    "    references.append(reference_answer)\n",
    "\n",
    "# Evaluation metrics computation (Accuracy, F1 Score, Precision, Recall, Jaccard Similarity, ROUGE Score, BLEU)\n",
    "def compute_metrics(predictions, references):\n",
    "    # Exact Match\n",
    "    exact_matches = [1 if pred == ref else 0 for pred, ref in zip(predictions, references)]\n",
    "    accuracy = sum(exact_matches) / len(exact_matches)\n",
    "\n",
    "    # Precision and Recall\n",
    "    precision = precision_score(references, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(references, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = f1_score(references, predictions, average='macro')\n",
    "\n",
    "\n",
    "    # BLEU Score with smoothing\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        reference = [ref.split()]  # BLEU expects a list of references\n",
    "        candidate = pred.split()\n",
    "        bleu = sentence_bleu(reference, candidate, weights=(0.5, 0.5), smoothing_function=smoothing_function)\n",
    "        bleu_scores.append(bleu)\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "\n",
    "    # ROUGE Score\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = [rouge_scorer_instance.score(ref, pred) for ref, pred in zip(references, predictions)]\n",
    "    rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores])\n",
    "    rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores])\n",
    "    rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores])\n",
    "\n",
    "    return accuracy, f1, precision, recall, avg_bleu, (rouge1, rouge2, rougeL)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy, f1, precision, recall, avg_bleu, rouge_scores = compute_metrics(predictions, references)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'BLEU Score: {avg_bleu}')\n",
    "print(f'ROUGE-L: {rouge_scores[2]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe0668",
   "metadata": {},
   "source": [
    "# 4. Supplemental Imports for Hybrid Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6da879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e51246",
   "metadata": {},
   "source": [
    "# 5. Evaluate Hybrid Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e474b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b938e88d6144a59c28e66cb1d88ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:03<00:00, 32.17it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:04<00:00, 22.76it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:08<00:00, 12.35it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:06<00:00, 14.74it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:03<00:00, 33.12it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:04<00:00, 23.30it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:04<00:00, 20.68it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 19.10it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:03<00:00, 32.70it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:04<00:00, 23.21it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:04<00:00, 20.59it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.84it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:03<00:00, 32.40it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:04<00:00, 23.18it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:04<00:00, 20.60it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 19.03it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:03<00:00, 32.79it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:04<00:00, 23.14it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:04<00:00, 20.54it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha: 0.1, Best k: 3\n",
      "Accuracy: 64.00%\n",
      "F1 Score: 0.42\n",
      "Precision: 0.42\n",
      "Recall: 0.47\n",
      "ROUGE-L: 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the SQuAD dataset\n",
    "squad = load_dataset('squad', split='validation')\n",
    "squad = squad.select(range(100))\n",
    "\n",
    "# SentenceTransformer model for embedding\n",
    "retrieval_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "documents = squad['context']\n",
    "document_embeddings = retrieval_model.encode(documents, show_progress_bar=True)\n",
    "\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(document_embeddings)\n",
    "\n",
    "# TF-IDF for Sparse Retrieval\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# QA pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", device=0)\n",
    "\n",
    "# Hybrid concatenation mechanism\n",
    "def hybrid_rag_answer(question, k=3, alpha=0.7, max_context_length=512):\n",
    "    # Dense Retrieval\n",
    "    question_embedding = retrieval_model.encode([question])\n",
    "    distances, indices = index.search(question_embedding, k)\n",
    "    dense_scores = 1 / (1 + distances)  # Convert distances to similarity scores\n",
    "    \n",
    "    # Sparse Retrieval\n",
    "    query_tfidf = tfidf_vectorizer.transform([question])\n",
    "    sparse_scores = (query_tfidf @ tfidf_matrix.T).toarray().flatten()\n",
    "    \n",
    "    # Combine Dense and Sparse Scores for the top k vector score retrievals\n",
    "    sparse_scores_k = sparse_scores[indices.flatten()]\n",
    "    combined_scores = alpha * dense_scores.flatten() + (1 - alpha) * sparse_scores_k\n",
    "    \n",
    "    # Retrieve the top k similar documents\n",
    "    top_k_indices = indices.flatten()[np.argsort(combined_scores)[-k:][::-1]]\n",
    "    retrieved_docs = [documents[i] for i in top_k_indices]\n",
    "    \n",
    "    context = \" \".join(retrieved_docs)\n",
    "    if len(context.split()) > max_context_length:\n",
    "        context = \" \".join(context.split()[:max_context_length])\n",
    "    \n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    \n",
    "    return result['answer']\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Evaluation metrics computation (Accuracy, F1 Score, Precision, Recall, MRR, Jaccard Similarity, ROUGE Score)\n",
    "def compute_metrics(predictions, references):\n",
    "    # Exact Match\n",
    "    exact_matches = [1 if pred == ref else 0 for pred, ref in zip(predictions, references)]\n",
    "    accuracy = sum(exact_matches) / len(exact_matches)\n",
    "\n",
    "    # Precision and Recall\n",
    "    precision = precision_score(references, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(references, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = f1_score(references, predictions, average='macro')\n",
    "\n",
    "    # Mean Reciprocal Rank (MRR)\n",
    "    reciprocal_ranks = []\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        if pred in ref:\n",
    "            reciprocal_ranks.append(1 / (references.index(ref) + 1))\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)\n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "\n",
    "    # Jaccard Similarity\n",
    "    jaccard_scores = []\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        set_ref = set(ref.split())\n",
    "        set_pred = set(pred.split())\n",
    "        jaccard_score = len(set_ref.intersection(set_pred)) / len(set_ref.union(set_pred)) if len(set_ref.union(set_pred)) > 0 else 0\n",
    "        jaccard_scores.append(jaccard_score)\n",
    "    avg_jaccard = np.mean(jaccard_scores)\n",
    "\n",
    "    # ROUGE Score\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = [rouge_scorer_instance.score(ref, pred) for ref, pred in zip(references, predictions)]\n",
    "    rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores])\n",
    "    rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores])\n",
    "    rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores])\n",
    "\n",
    "    return accuracy, f1, precision, recall, mrr, avg_jaccard, (rouge1, rouge2, rougeL)\n",
    "\n",
    "\n",
    "# Hyperparameter Tuning (alpha, documents)\n",
    "best_alpha = 0.5\n",
    "best_k = 5\n",
    "best_accuracy = 0\n",
    "best_f1 = 0\n",
    "\n",
    "# Variables to hold the best metric scores\n",
    "best_metrics = None\n",
    "\n",
    "for alpha in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    for k in [3, 5, 7, 9]:\n",
    "        predictions = []\n",
    "        references = []\n",
    "\n",
    "        for example in tqdm(squad):\n",
    "            question = example['question']\n",
    "            reference_answer = example['answers']['text'][0]\n",
    "            predicted_answer = hybrid_rag_answer(question, k=k, alpha=alpha)\n",
    "            \n",
    "            predictions.append(predicted_answer)\n",
    "            references.append(reference_answer)\n",
    "\n",
    "        accuracy, f1, precision, recall, mrr, avg_jaccard, rouge_scores = compute_metrics(predictions, references)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_alpha = alpha\n",
    "            best_k = k\n",
    "            best_accuracy = accuracy\n",
    "            best_f1 = f1\n",
    "            best_metrics = (accuracy, f1, precision, recall, mrr, avg_jaccard, rouge_scores)\n",
    "\n",
    "print(f'Best Alpha: {best_alpha}, Best k: {best_k}')\n",
    "# Print only the best metrics\n",
    "if best_metrics:\n",
    "    best_accuracy, best_f1, best_precision, best_recall, best_mrr, best_avg_jaccard, best_rouge = best_metrics\n",
    "    best_rouge1, best_rouge2, best_rougeL = best_rouge\n",
    "    print(f'Accuracy: {best_accuracy * 100:.2f}%')\n",
    "    print(f'F1 Score: {best_f1:.2f}')\n",
    "    print(f'Precision: {best_precision:.2f}')\n",
    "    print(f'Recall: {best_recall:.2f}')\n",
    "    print(f'ROUGE-L: {best_rougeL:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
